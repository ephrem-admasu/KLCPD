{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd0ad2c-0d21-4a56-826d-4fbd69d4b1cf",
   "metadata": {},
   "source": [
    "**UTILITS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1f6f5-5061-4cc3-aa77-7f9a1d3cc056",
   "metadata": {
    "tags": []
   },
   "source": [
    "This code defines a machine learning framework for a specific type of time series analysis, likely intended for anomaly detection or change point detection, using a generative adversarial network (GAN) approach. The main components and their roles are as follows:\n",
    "\n",
    "**Environment Setup**: Imports necessary libraries and modules, loads environment variables for directory paths (e.g., model saving and predictions directories), and sets the computing device (GPU or CPU).\n",
    "\n",
    "**Functions and Classes**:\n",
    "\n",
    "**median_heuristic**: Calculates a set of scales based on the median of pairwise squared distances in the input data. This is often used to set the bandwidth for kernel methods, such as the Gaussian kernel in Maximum Mean Discrepancy (MMD) calculations.\n",
    "\n",
    "**NetG and NetD**: Define the generator and discriminator networks for the GAN, using GRU (Gated Recurrent Unit) layers for sequence processing. The generator synthesizes time series data, while the discriminator attempts to distinguish between real and generated data.\n",
    "\n",
    "**KL_CPD**: The main class implementing the GAN-based change point detection model. It includes methods for model training (fit), prediction (predict), and loss visualization (plot_losses). It utilizes MMD for comparing distributions of real and generated data as part of its training process.\n",
    "\n",
    "**svd_wrapper**: A helper function to perform Singular Value Decomposition (SVD) using various methods (svds, randomized_svd, or np.linalg.svd). SVD is a dimensionality reduction technique that can also denoise data.\n",
    "\n",
    "**get_reduced_data**: Reduces the dimensionality of the input dataset using SVD, which can help in processing high-dimensional time series data more efficiently.\n",
    "\n",
    "**train_and_pred_dataset**: Orchestrates the training and prediction process for a given dataset, including handling model saving/loading based on the provided dataset name and SVD parameters.\n",
    "\n",
    "**save_preds**: Visualizes and saves the predictions made by the model, alongside the components of the original dataset for comparison.\n",
    "\n",
    "**GAN Training and Prediction Workflow**:\n",
    "\n",
    "The model, specified by KL_CPD, is trained on a time series dataset. It first optionally reduces the dimensionality of the data using SVD.The training involves alternating between updating the discriminator (NetD) and generator (NetG) networks. The discriminator learns to differentiate between real and generated (synthetic) sequences, while the generator learns to produce sequences that are indistinguishable from real data.\n",
    "\n",
    "The adversarial training process is guided by a loss function that incorporates MMD, a measure of the distance between the distributions of real and generated data.z\n",
    "\n",
    "**Saving and Loading Models**: The framework supports saving the state of the model during training, which allows for training interruption and resumption without starting over.\n",
    "\n",
    "**Utility and Visualization**: The code includes utilities for data preprocessing (e.g., HankelDataset for organizing time series data into training batches) and for visualizing training losses and prediction results.\n",
    "\n",
    "Overall, this script is a comprehensive framework for training a GAN-based model on time series data for the purpose of detecting changes or anomalies. It leverages advanced techniques like RNNs for sequence data processing and MMD for distribution comparison, and it includes thoughtful considerations for usability, such as model saving/loading and visualization of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5eb6cae-cf29-4b19-8626-2d9bf3b7d8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class HankelDataset(Dataset):\n",
    "    def __init__(self, ts: np.array, p_wnd_dim:int=25, f_wnd_dim:int=10, sub_dim:int=1):\n",
    "        \"\"\"\n",
    "        @param ts - timeseries\n",
    "        @param p_wnd_dim - past window size\n",
    "        @param f_wnd_dim - future window size\n",
    "        @param sub_dim - Hankel matrix size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.p_wnd_dim = p_wnd_dim\n",
    "        self.f_wnd_dim = f_wnd_dim\n",
    "        self.sub_dim = sub_dim\n",
    "\n",
    "        if len(ts.shape) == 1: \n",
    "            ts = np.expand_dims(ts, -1)\n",
    "        self.Y = ts # Timeseries\n",
    "        self.T, self.D = ts.shape # T: time length; D: variable dimension\n",
    "        self.var_dim = self.D * self.sub_dim\n",
    "\n",
    "        self.Y_hankel = self.ts_to_hankel(self.Y)\n",
    "\n",
    "    # prepare subspace data (Hankel matrix)\n",
    "    def ts_to_hankel(self, ts):\n",
    "        # T x D x sub_dim\n",
    "        Y_hankel = np.zeros((self.T, self.D, self.sub_dim))\n",
    "        for t in range(self.sub_dim, self.T):\n",
    "            for d in range(self.D):\n",
    "                Y_hankel[t, d, :] = ts[t-self.sub_dim+1:t+1, d].flatten()\n",
    "\n",
    "        # Y_hankel is now T x (Dxsub_dim)\n",
    "        Y_hankel = Y_hankel.reshape(self.T, -1)\n",
    "        return Y_hankel\n",
    "\n",
    "\n",
    "    # convert augmented data in Hankel matrix to origin time series\n",
    "    # input: X_f, whose shape is batch_size x seq_len x (D*sub_dim)\n",
    "    # output: Y_t, whose shape is batch_size x D\n",
    "    def hankel_to_ts(self, X_f):\n",
    "        batch_size = X_f.shape[0]\n",
    "        Y_t = X_f[:, 0, :].contiguous().view(batch_size, self.D, self.sub_dim)\n",
    "        return Y_t[:, :, -1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0: \n",
    "            idx += len(self)\n",
    "        assert(0 <= idx < len(self))\n",
    "        data = np.concatenate([\n",
    "            np.tile(self.Y_hankel[:1,:], (self.p_wnd_dim, 1)), # left padding\n",
    "            self.Y_hankel[:, :], # timeseries\n",
    "            np.tile(self.Y_hankel[-1:,:], (self.f_wnd_dim, 1)), # right padding\n",
    "            ])\n",
    "        return {\n",
    "            'X_p': torch.from_numpy(data[idx:idx+self.p_wnd_dim, :]),\n",
    "            'X_f': torch.from_numpy(data[idx+self.p_wnd_dim:idx+self.p_wnd_dim+self.f_wnd_dim, :]),\n",
    "            'Y': torch.from_numpy(self.Y[min(max(idx, 0), self.T-1)])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a702991e-a1c9-404e-a8dc-8ccbf2a99883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.utils.extmath import svd_flip, randomized_svd\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt\n",
    "#from .data import HankelDataset\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d77eae6-82f1-4abe-963f-9beda3c47d28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Running on {device.upper()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8d43f34-999b-422a-87fe-c9bc3b29f471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file saved to: paths\\predictions\\test.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the paths\n",
    "base_dir = 'paths'\n",
    "SAVE_MODEL_DIR_PATH =  os.path.join(base_dir, 'models')\n",
    "PREDS_DIR_PATH =  os.path.join(base_dir, 'predictions')\n",
    "\n",
    "# Example: Save a dummy file in the predictions directory to test\n",
    "test_file_path = os.path.join(PREDS_DIR_PATH, 'test.txt')\n",
    "with open(test_file_path, 'w') as file:\n",
    "    file.write('This is a test file.')\n",
    "\n",
    "print(f'Test file saved to: {test_file_path}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05c174f8-311d-403c-9c3f-1ec72060836d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def median_heuristic(X, beta=0.5):\n",
    "    max_n = min(30000, X.shape[0])\n",
    "    D2 = euclidean_distances(X[:max_n], squared=True)\n",
    "    med_sqdist = np.median(D2[np.triu_indices_from(D2, k=1)])\n",
    "    beta_list = [beta**2, beta**1, 1, (1.0/beta)**1, (1.0/beta)**2]\n",
    "    return [med_sqdist * b for b in beta_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f59c16be-c99c-4a47-9605-d6013c950222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NetG(nn.Module):\n",
    "    def __init__(self, var_dim, RNN_hid_dim, num_layers: int = 1):\n",
    "        super().__init__()\n",
    "        self.var_dim = var_dim\n",
    "        self.RNN_hid_dim = RNN_hid_dim\n",
    "\n",
    "        self.rnn_enc_layer = nn.GRU(\n",
    "            self.var_dim, self.RNN_hid_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.rnn_dec_layer = nn.GRU(\n",
    "            self.var_dim, self.RNN_hid_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc_layer = nn.Linear(self.RNN_hid_dim, self.var_dim)\n",
    "\n",
    "    def forward(self, X_p, X_f, noise):\n",
    "        X_p_enc, h_t = self.rnn_enc_layer(X_p)\n",
    "        X_f_shft = self.shft_right_one(X_f)\n",
    "        hidden = h_t + noise\n",
    "        Y_f, _ = self.rnn_dec_layer(X_f_shft, hidden)\n",
    "        output = self.fc_layer(Y_f)\n",
    "        return output\n",
    "\n",
    "    def shft_right_one(self, X):\n",
    "        X_shft = X.clone()\n",
    "        X_shft[:, 0, :].data.fill_(0)\n",
    "        X_shft[:, 1:, :] = X[:, :-1, :]\n",
    "        return X_shft\n",
    "\n",
    "\n",
    "class NetD(nn.Module):\n",
    "    def __init__(self, var_dim, RNN_hid_dim, num_layers: int = 1):\n",
    "        super(NetD, self).__init__()\n",
    "\n",
    "        self.var_dim = var_dim\n",
    "        self.RNN_hid_dim = RNN_hid_dim\n",
    "\n",
    "        self.rnn_enc_layer = nn.GRU(\n",
    "            self.var_dim, self.RNN_hid_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.rnn_dec_layer = nn.GRU(\n",
    "            self.RNN_hid_dim, self.var_dim, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_enc, _ = self.rnn_enc_layer(X)\n",
    "        X_dec, _ = self.rnn_dec_layer(X_enc)\n",
    "        return X_enc, X_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dfffd98-3774-4d25-ad9d-32e1f0937a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KL_CPD(nn.Module):\n",
    "    def __init__(self, D: int, critic_iters: int = 5,\n",
    "                 lambda_ae: float = 1e-5, lambda_real: float = 1e-3,\n",
    "                 p_wnd_dim: int = 3, f_wnd_dim: int = 2, sub_dim: int = 1, RNN_hid_dim: int = 15):\n",
    "        super().__init__()\n",
    "        self.p_wnd_dim = p_wnd_dim\n",
    "        self.f_wnd_dim = f_wnd_dim\n",
    "        self.sub_dim = sub_dim\n",
    "        self.D = D\n",
    "        self.var_dim = D * sub_dim\n",
    "        self.critic_iters = critic_iters\n",
    "        self.lambda_ae, self.lambda_real = lambda_ae, lambda_real\n",
    "        self.RNN_hid_dim = RNN_hid_dim\n",
    "        self.netD = NetD(self.var_dim, RNN_hid_dim)\n",
    "        self.netG = NetG(self.var_dim, RNN_hid_dim)\n",
    "        self.loss_g_list = []\n",
    "        self.loss_d_list = []\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __mmd2_loss(self, X_p_enc, X_f_enc):\n",
    "        sigma_var = self.sigma_var\n",
    "\n",
    "        # some constants\n",
    "        n_basis = 1024\n",
    "        gumbel_lmd = 1e+6\n",
    "        cnst = math.sqrt(1. / n_basis)\n",
    "        n_mixtures = sigma_var.size(0)\n",
    "        n_samples = n_basis * n_mixtures\n",
    "        batch_size, seq_len, nz = X_p_enc.size()\n",
    "\n",
    "        # gumbel trick to get masking matrix to uniformly sample sigma\n",
    "        # input: (batch_size*n_samples, nz)\n",
    "        # output: (batch_size, n_samples, nz)\n",
    "        def sample_gmm(W, batch_size):\n",
    "            U = torch.FloatTensor(batch_size*n_samples,\n",
    "                                  n_mixtures).uniform_().to(self.device)\n",
    "            sigma_samples = F.softmax(U * gumbel_lmd, dim=1).matmul(sigma_var)\n",
    "            W_gmm = W.mul(1. / sigma_samples.unsqueeze(1))\n",
    "            W_gmm = W_gmm.view(batch_size, n_samples, nz)\n",
    "            return W_gmm\n",
    "\n",
    "        W = Variable(torch.FloatTensor(batch_size*n_samples,\n",
    "                     nz).normal_(0, 1).to(self.device))\n",
    "        # batch_size x n_samples x nz\n",
    "        W_gmm = sample_gmm(W, batch_size)\n",
    "        # batch_size x nz x n_samples\n",
    "        W_gmm = torch.transpose(W_gmm, 1, 2).contiguous()\n",
    "        # batch_size x seq_len x n_samples\n",
    "        XW_p = torch.bmm(X_p_enc, W_gmm)\n",
    "        # batch_size x seq_len x n_samples\n",
    "        XW_f = torch.bmm(X_f_enc, W_gmm)\n",
    "        z_XW_p = cnst * torch.cat((torch.cos(XW_p), torch.sin(XW_p)), 2)\n",
    "        z_XW_f = cnst * torch.cat((torch.cos(XW_f), torch.sin(XW_f)), 2)\n",
    "        batch_mmd2_rff = torch.sum((z_XW_p.mean(1) - z_XW_f.mean(1))**2, 1)\n",
    "        return batch_mmd2_rff\n",
    "\n",
    "    def forward(self, X_p: torch.Tensor, X_f: torch.Tensor):\n",
    "        batch_size = X_p.size(0)\n",
    "\n",
    "        X_p_enc, _ = self.netD(X_p)\n",
    "        X_f_enc, _ = self.netD(X_f)\n",
    "        Y_pred_batch = self.__mmd2_loss(X_p_enc, X_f_enc)\n",
    "\n",
    "        return Y_pred_batch\n",
    "\n",
    "    def predict(self, ts):\n",
    "        dataset = HankelDataset(\n",
    "            ts, self.p_wnd_dim, self.f_wnd_dim, self.sub_dim)\n",
    "        dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                X_p, X_f = [batch[key].float().to(self.device)\n",
    "                            for key in ['X_p', 'X_f']]\n",
    "                pred_val = self.forward(X_p, X_f).cpu().detach().numpy()\n",
    "                preds.append(pred_val)\n",
    "        return np.concatenate(preds)\n",
    "\n",
    "    def fit(self, ts, start_epoch, svd_method, components, epoches: int = 100, lr: float = 1e-2, weight_clip: float = .1, weight_decay: float = 0., momentum: float = 0., dataset_name=None):\n",
    "        print('***** Training *****')\n",
    "        # must be defined in fit() method\n",
    "        optG_adam = torch.optim.AdamW(\n",
    "            self.netG.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        # lr_scheduler_g = lr_scheduler.CosineAnnealingLR(optG, T_max=epoches, eta_min=3e-5)\n",
    "        optD_adam = torch.optim.AdamW(\n",
    "            self.netD.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        # lr_scheduler_d = lr_scheduler.CosineAnnealingLR(optD, T_max=epoches, eta_min=3e-5)\n",
    "        optD_rmsprop = torch.optim.RMSprop(\n",
    "            self.netD.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        optG_rmsprop = torch.optim.RMSprop(\n",
    "            self.netG.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        \n",
    "\n",
    "        dataset = HankelDataset(\n",
    "            ts, self.p_wnd_dim, self.f_wnd_dim, self.sub_dim)\n",
    "        dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "        sigma_list = median_heuristic(dataset.Y_hankel, beta=.5)\n",
    "        self.sigma_var = torch.FloatTensor(sigma_list).to(self.device)\n",
    "\n",
    "        # tbar = trange(epoches)\n",
    "        for epoch in tqdm(range(start_epoch, epoches)):\n",
    "            for batch in dataloader:\n",
    "                # Fit critic\n",
    "                for p in self.netD.parameters():\n",
    "                    p.requires_grad = True\n",
    "                for p in self.netD.rnn_enc_layer.parameters():\n",
    "                    p.data.clamp_(-weight_clip, weight_clip)\n",
    "                # (D_mmd2_mean, mmd2_real_mean, real_L2_loss, fake_L2_loss) = self._optimizeD(batch, optD)\n",
    "                # train after every 15 epochs\n",
    "                # if epoch % 15 == 0:\n",
    "                self._optimizeD(batch, optD_rmsprop)\n",
    "                # G_mmd2_mean = 0\n",
    "                # if np.random.choice(np.arange(self.critic_iters)) == 0:\n",
    "                # Fit generator\n",
    "                for p in self.netD.parameters():\n",
    "                    p.requires_grad = False  # to avoid computation\n",
    "                # G_mmd2_mean = self._optimizeG(batch, optG)\n",
    "                self._optimizeG(batch, optG_rmsprop)\n",
    "            \n",
    "            optD_adam.step()\n",
    "            optG_adam.step()          \n",
    "\n",
    "            # saving model dict to file after every 5 epochs\n",
    "            if dataset_name:\n",
    "                if epoch % 2 == 0:\n",
    "                    torch.save(self.netD.state_dict(\n",
    "                    ), f'{SAVE_MODEL_DIR_PATH}/{dataset_name}/{svd_method}_{components}/netd_{epoch}.pt')\n",
    "                    torch.save(self.netG.state_dict(\n",
    "                    ), f'{SAVE_MODEL_DIR_PATH}/{dataset_name}/{svd_method}_{components}/netg_{epoch}.pt')\n",
    "        print('***** Plotting losses for Generator and Discriminator models *****')\n",
    "        self.plot_losses(reduction_method=svd_method,\n",
    "                         components=components, dataset_name=dataset_name)\n",
    "        # print('[%5d/%5d] D_mmd2 %.4e G_mmd2 %.4e mmd2_real %.4e real_L2 %.6f fake_L2 %.6f'\n",
    "        #   % (epoch+1, epoches, D_mmd2_mean, G_mmd2_mean, mmd2_real_mean, real_L2_loss, fake_L2_loss))\n",
    "\n",
    "    def _optimizeG(self, batch, opt, lr_scheduler=None, grad_clip: int = 5):\n",
    "        X_p, X_f = [batch[key].float().to(self.device)\n",
    "                    for key in ['X_p', 'X_f']]\n",
    "        batch_size = X_p.size(0)\n",
    "\n",
    "        # real data\n",
    "        X_f_enc, X_f_dec = self.netD(X_f)\n",
    "\n",
    "        # fake data\n",
    "        noise = torch.FloatTensor(1, batch_size, self.RNN_hid_dim).uniform_(-1, 1).to(self.device)\n",
    "        # noise = torch.FloatTensor(1, batch_size, self.RNN_hid_dim).normal_(0, 1).to(self.device)\n",
    "        noise = Variable(noise)\n",
    "        Y_f = self.netG(X_p, X_f, noise)\n",
    "        Y_f_enc, Y_f_dec = self.netD(Y_f)\n",
    "\n",
    "        # batchwise MMD2 loss between X_f and Y_f\n",
    "        G_mmd2 = self.__mmd2_loss(X_f_enc, Y_f_enc)\n",
    "\n",
    "        # update netG\n",
    "        self.netG.zero_grad()\n",
    "        lossG = G_mmd2.mean()\n",
    "        # lossG = 0.0 * G_mmd2.mean()\n",
    "        self.loss_g_list.append(lossG.data.item())\n",
    "        lossG.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(self.netG.parameters(), grad_clip)\n",
    "\n",
    "        opt.step()\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        # return G_mmd2.mean().data.item()\n",
    "\n",
    "    def _optimizeD(self, batch, opt, lr_scheduler=None, grad_clip: int = 5):\n",
    "        X_p, X_f, Y_true = [batch[key].float().to(self.device)\n",
    "                            for key in ['X_p', 'X_f', 'Y']]\n",
    "        batch_size = X_p.size(0)\n",
    "\n",
    "        # real data\n",
    "        X_p_enc, X_p_dec = self.netD(X_p)\n",
    "        X_f_enc, X_f_dec = self.netD(X_f)\n",
    "\n",
    "        # fake data\n",
    "        noise = torch.FloatTensor(1, batch_size, self.netG.RNN_hid_dim).uniform_(-1, 1).to(self.device)\n",
    "        # noise = torch.FloatTensor(1, batch_size, self.netG.RNN_hid_dim).normal_(0, 1).to(self.device)\n",
    "        noise = Variable(noise)  # total freeze netG\n",
    "        torch.no_grad()\n",
    "        Y_f = Variable(self.netG(X_p, X_f, noise).data)\n",
    "        Y_f_enc, Y_f_dec = self.netD(Y_f)\n",
    "\n",
    "        # batchwise MMD2 loss between X_f and Y_f\n",
    "        D_mmd2 = self.__mmd2_loss(X_f_enc, Y_f_enc)\n",
    "\n",
    "        # batchwise MMD loss between X_p and X_f\n",
    "        mmd2_real = self.__mmd2_loss(X_p_enc, X_f_enc)\n",
    "\n",
    "        # reconstruction loss\n",
    "        real_L2_loss = torch.mean((X_f - X_f_dec)**2)\n",
    "        fake_L2_loss = torch.mean((Y_f - Y_f_dec)**2)\n",
    "\n",
    "        # update netD\n",
    "        self.netD.zero_grad()\n",
    "        lossD = D_mmd2.mean() - self.lambda_ae * (real_L2_loss + fake_L2_loss) - \\\n",
    "            self.lambda_real * mmd2_real.mean()\n",
    "        lossD = -lossD\n",
    "        self.loss_d_list.append(lossD.data.item())\n",
    "        lossD.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(self.netD.parameters(), grad_clip)\n",
    "\n",
    "        opt.step()\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        # return D_mmd2.mean().data.item(), mmd2_real.mean().data.item(), real_L2_loss.data.item(), fake_L2_loss.data.item()\n",
    "\n",
    "    def plot_losses(self, reduction_method: str, components: int, dataset_name: str):\n",
    "        curr_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.loss_g_list, label='loss_g')\n",
    "        plt.plot(self.loss_d_list, label='loss_d')\n",
    "        plt.legend()\n",
    "        plt.savefig(\n",
    "            f'{PREDS_DIR_PATH}/{curr_time}_{reduction_method}_{components}_{dataset_name}_loss.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7d889cc-39f8-483c-8916-138861b713b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def svd_wrapper(Y, k, method='svds'):\n",
    "    if method == 'svds':\n",
    "        Ut, St, Vt = svds(Y, k)\n",
    "        idx = np.argsort(St)[::-1]\n",
    "        St = St[idx]  # have issue with sorting zero singular values\n",
    "        Ut, Vt = svd_flip(Ut[:, idx], Vt[idx])\n",
    "    elif method == 'random':\n",
    "        Ut, St, Vt = randomized_svd(Y, k, random_state=0)\n",
    "    else:\n",
    "        Ut, St, Vt = np.linalg.svd(Y, full_matrices=False)\n",
    "        # now truncate it to k\n",
    "        Ut = Ut[:, :k]\n",
    "        St = np.diag(St[:k])\n",
    "        Vt = Vt[:k, :]\n",
    "\n",
    "    return Ut, St, Vt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd072743-7348-44c2-a3f5-a4372f99fa58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_reduced_data(dataset, components, svd_method):\n",
    "    print(f'***** Original dataset shape: {dataset.shape} *****')\n",
    "    X, _, _ = svd_wrapper(dataset, components, method=svd_method)\n",
    "    print(f'***** Reduced dataset shape: {X.shape} *****')\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bddeda23-a0dc-4868-bede-a61b2f602ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_pred_dataset(dataset, dataset_name, svd_method, components, preload_model=False):\n",
    "    '''If dataset name is not None, then save the model dict to file after each epoch. If preload_model is True, then load the model from file and continue training'''\n",
    "    dimension = dataset.shape[1]\n",
    "    start_epoch = 0\n",
    "    model = KL_CPD(dimension).to(device)\n",
    "\n",
    "    # get model state dict from the file\n",
    "    if dataset_name:\n",
    "        # check if folder exists if not then create it\n",
    "        model_folder_path = f'{SAVE_MODEL_DIR_PATH}/{dataset_name}/{svd_method}_{components}/'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "        dir_files = os.listdir(model_folder_path)\n",
    "        print('-----------------', len(dir_files))\n",
    "        # check if folder is empty and check if reset_model_folder is False\n",
    "        if len(dir_files) > 0 and preload_model:\n",
    "            # sort files that starts with netd and netg\n",
    "            netd_files = [\n",
    "                file for file in dir_files if file.startswith('netd')]\n",
    "            netg_files = [\n",
    "                file for file in dir_files if file.startswith('netg')]\n",
    "\n",
    "            # sort files with latest timestamp\n",
    "            netd_files = sorted(netd_files, key=lambda x: os.path.getmtime(\n",
    "                model_folder_path + x), reverse=True)\n",
    "            netg_files = sorted(netg_files, key=lambda x: os.path.getmtime(\n",
    "                model_folder_path + x), reverse=True)\n",
    "\n",
    "            # load the latest file from netd and netg\n",
    "            net_d_params = torch.load(model_folder_path + netd_files[0])\n",
    "            net_g_params = torch.load(model_folder_path + netg_files[0])\n",
    "\n",
    "            # load params to model\n",
    "            model.netD.load_state_dict(net_d_params)\n",
    "            model.netG.load_state_dict(net_g_params)\n",
    "\n",
    "            # get the epoch number from the model file name. if epoch number is different then choose the min epoch number\n",
    "            start_epoch = min(int(netd_files[0].split(\n",
    "                '_')[-1].split('.')[0]), int(netg_files[0].split('_')[-1].split('.')[0]))\n",
    "            print(\n",
    "                f'***** Loaded model from file: {svd_method}_{components}/{netd_files[0]} and {svd_method}_{components}/{netg_files[0]} with epoch {start_epoch} *****')\n",
    "\n",
    "    model.fit(dataset, dataset_name=dataset_name, start_epoch=start_epoch,\n",
    "              svd_method=svd_method, components=components)\n",
    "    predictions = model.predict(dataset)\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c21331f-4d6d-43c2-bf58-f207bb49459c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_preds(dataset, predictions, reduction_method, dataset_name, skip_components=0, save_preds=True):\n",
    "    print('***** Saving Predictions *****')\n",
    "    components = dataset.shape[1]\n",
    "    # get the min and max values for y-axis\n",
    "    min_y = float('inf')\n",
    "    max_y = float('-inf')\n",
    "    for i in range(components):\n",
    "        if skip_components == i+1:\n",
    "            continue\n",
    "        min_y = min(min_y, min(dataset[:, i]))\n",
    "        max_y = max(max_y, max(dataset[:, i]))\n",
    "\n",
    "    for i in range(components):\n",
    "        if skip_components == i+1:\n",
    "            continue\n",
    "        plt.subplot(components+1, 1, i+1)\n",
    "        plt.plot(dataset[:, i])\n",
    "        plt.title(f'Component {i+1}')\n",
    "        plt.ylim([min_y-0.2, max_y+0.2])\n",
    "        plt.subplot(components+1, 1, components+1)\n",
    "\n",
    "    plt.plot(predictions)\n",
    "    plt.title('MMD')\n",
    "    plt.suptitle(\n",
    "        f'{reduction_method} with {components-skip_components} component(s) visualization')\n",
    "    plt.tight_layout()\n",
    "    if save_preds:\n",
    "        curr_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        plt.savefig(\n",
    "            f'{PREDS_DIR_PATH}/{curr_time}_{reduction_method}_{components-skip_components}_{dataset_name}.png')\n",
    "    plt.show()\n",
    "    print('***** DONE *****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d5825f9-d4bf-48c5-b248-616df0602120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class DatasetName(Enum):\n",
    "    PROTEIN_1BYZ = '1byz_md.xyz', [100, 56, 3]\n",
    "    \n",
    "def check_valid_dataset(dataset_name):\n",
    "    assert isinstance(dataset_name, str) and dataset_name in DatasetName.__members__, 'Dataset name must be a string'\n",
    "    return True\n",
    "\n",
    "def get_details(dataset_name):\n",
    "    check_valid_dataset(dataset_name)\n",
    "    data = DatasetName[dataset_name].value\n",
    "    return data\n",
    "\n",
    "\n",
    "def check_pkl_file(file_name):\n",
    "    file_arr = file_name.split('.')\n",
    "    pkl_file_name = '_'.join(file_arr) + '.pkl'\n",
    "    files = os.listdir(DATASET_DIR_PATH)\n",
    "\n",
    "    return pkl_file_name in files, f'{DATASET_DIR_PATH}/{pkl_file_name}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fece1bea-e33f-4bd6-b3c6-a1ba5dfada0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coordinates(dataset_name):\n",
    "    file_name, shape = get_details(dataset_name)\n",
    "    is_present, pickle_file_name = check_pkl_file(file_name)\n",
    "\n",
    "    if is_present:\n",
    "        coordinates = torch.load(pickle_file_name)\n",
    "        return coordinates\n",
    "    \n",
    "    xyz = open(f'{DATASET_DIR_PATH}/{file_name}')\n",
    "    \n",
    "    coordinates = []\n",
    "    for row in xyz:\n",
    "        x, y, z = row.split(',')\n",
    "        coordinates.append([float(x), float(y), float(z)])\n",
    "\n",
    "    coordinates = np.array(coordinates)\n",
    "    coordinates = coordinates.reshape(shape)\n",
    "\n",
    "    (x1, y1, z1) = coordinates.shape\n",
    "    coordinates = coordinates.reshape(x1, y1*z1, order='C')\n",
    "\n",
    "    torch.save(coordinates, pickle_file_name)\n",
    "\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42e74457-f2bc-4c08-bbc1-bc1fc643a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(dataset_name, svd_method, components):\n",
    "#     data = get_coordinates(dataset_name)\n",
    "#     data_reduced = get_reduced_data(data, components, svd_method)\n",
    "#     preds = train_and_pred_dataset(data_reduced, dataset_name.lower(), svd_method, components)\n",
    "#     save_preds(data_reduced, preds, svd_method, dataset_name.lower())\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     dataset_name = '1byz_md.xyz'\n",
    "#     svd_method = 'random'\n",
    "#     components = 2\n",
    "#     train(dataset_name, svd_method, components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "829eb7b4-b4f8-4fad-93c0-43b4ab3b46b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Assuming your .xyz file is located directly in the \"1byz_md.xyz\" folder,\n",
    "# and your directory structure is \"/pscratch/sd/s/saik1999/KLCPD/protein_data/1byz_md.xyz/\"\n",
    "DATASET_DIR_PATH = 'paths/data' # Path('paths/data')\n",
    "\n",
    "# The file name should be just the name of the file, not the entire path\n",
    "file_name = '1byz_md.xyz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6144bde-2a8d-4982-bda4-1de8763080aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paths/data'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_DIR_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b25d4191-b858-4c2b-8ce4-8e697337d5be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DATASET_DIR_PATH='pscratch/sd/s/saik1999/KLCPD/protein_data/1byz_md.xyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e74a3a65-e0d4-49bd-b305-7a6c871bd1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paths/data/1byz_md.xyz'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{DATASET_DIR_PATH}/{file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2102d1d3-7bbd-4376-9aa6-304677083264",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading XYZ file and creating pickle file.\n",
      "Loaded data shape: (5600, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming environment variables are set for the dataset directory path\n",
    "# DATASET_DIR_PATH = Path(os.getenv('DATASET_DIR_PATH', '.'))\n",
    "\n",
    "class DatasetName(Enum):\n",
    "    PROTEIN_1BYZ = '1byz_md.xyz', (100, 56, 3)\n",
    "\n",
    "def get_coordinates(dataset_name):\n",
    "    file_name, shape = DatasetName[dataset_name].value # dataset_enum.value\n",
    "    pkl_file_name = f\"{DATASET_DIR_PATH}/{file_name.split('.')[0]}.pkl\"\n",
    "\n",
    "    # if pkl_file_name.exists():\n",
    "    #     print(\"Loading coordinates from pickle file.\")\n",
    "    #     return torch.load(str(1pkl_file_name))\n",
    "    \n",
    "    print(\"Reading XYZ file and creating pickle file.\")\n",
    "    coordinates = []\n",
    "    with open(f\"{DATASET_DIR_PATH}/{file_name}\", 'r') as xyz_file:\n",
    "        # Skip the first line that possibly contains number of atoms\n",
    "        # next(xyz_file)\n",
    "        # Skip the second line that might be a comment\n",
    "        # next(xyz_file)\n",
    "        for row in xyz_file:\n",
    "            parts = row.split(',')\n",
    "            # print(parts)\n",
    "            if len(parts) == 4:  # Expecting: Atom X Y Z\n",
    "                # We are only interested in coordinates here, ignoring the atom type\n",
    "                coordinates.append(list(map(float, parts[1:])))\n",
    "    \n",
    "    # Assuming the 'shape' tuple contains the expected dimensions in your data\n",
    "    # For example, shape could be (1000, 3) for 1000 points in 3D space\n",
    "    L = len(coordinates)\n",
    "    D1 = 100\n",
    "    D2 = 56 # L // D1\n",
    "    coordinates = coordinates[:D1*D2]\n",
    "    coordinates = np.array(coordinates).reshape([D1, D2, 3])\n",
    "    coordinates = coordinates.reshape([D1 * D2, 3])\n",
    "    torch.save(coordinates, str(pkl_file_name))\n",
    "\n",
    "    return coordinates\n",
    "def main():\n",
    "    # Choose dataset\n",
    "    dataset_name = 'PROTEIN_1BYZ'\n",
    "    # print(dataset_enum, '--------')\n",
    "\n",
    "    # Get coordinates\n",
    "    coordinates = get_coordinates(dataset_name)\n",
    "\n",
    "    # Here you would add your model training code\n",
    "    print(f\"Loaded data shape: {coordinates.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "461b4dbe-d00f-44cc-9fcb-01c9dd13e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_name, svd_method, components):\n",
    "\n",
    "    preload_model_name = f\"PRE_LOAD_{dataset_name.upper()}_MODEL\"\n",
    "    # convert to boolean as env variable are always string\n",
    "    PRE_LOAD_PROTEIN_1FME_MODEL = os.getenv(preload_model_name, False) == 'True'\n",
    "    data = get_coordinates(dataset_name)\n",
    "    data_reduced = get_reduced_data(data, components, svd_method)\n",
    "    preds = train_and_pred_dataset(data_reduced, dataset_name.lower(), svd_method, components, preload_model=PRE_LOAD_PROTEIN_1FME_MODEL)\n",
    "    save_preds(data_reduced, preds, svd_method, dataset_name.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcbfaa5-5e6d-406a-b9fa-e79890542f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading XYZ file and creating pickle file.\n",
      "***** Original dataset shape: (5600, 3) *****\n",
      "***** Reduced dataset shape: (5600, 2) *****\n",
      "----------------- 0\n",
      "***** Training *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████████████████████████████████████████▏           | 85/100 [2:15:04<23:28, 93.91s/it]"
     ]
    }
   ],
   "source": [
    "dataset_name = 'PROTEIN_1BYZ'\n",
    "svd_method = 'random'\n",
    "components = 2\n",
    "train(dataset_name, svd_method, components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771394d6-a026-42a5-b598-c28bc8b0d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from hyperopt.pyll.base import scope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235489e3-17f5-4301-a6b8-78670040e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KL_CPD(nn.Module):\n",
    "    def __init__(self, D: int, critic_iters: int = 5,\n",
    "                 lambda_ae: float = 1e-5, lambda_real: float = 1e-3,\n",
    "                 p_wnd_dim: int = 3, f_wnd_dim: int = 2, sub_dim: int = 1, RNN_hid_dim: int = 15):\n",
    "        super().__init__()\n",
    "        self.p_wnd_dim = p_wnd_dim\n",
    "        self.f_wnd_dim = f_wnd_dim\n",
    "        self.sub_dim = sub_dim\n",
    "        self.D = D\n",
    "        self.var_dim = D * sub_dim\n",
    "        self.critic_iters = critic_iters\n",
    "        self.lambda_ae, self.lambda_real = lambda_ae, lambda_real\n",
    "        self.RNN_hid_dim = RNN_hid_dim\n",
    "        self.netD = NetD(self.var_dim, RNN_hid_dim)\n",
    "        self.netG = NetG(self.var_dim, RNN_hid_dim)\n",
    "        self.loss_g_list = []\n",
    "        self.loss_d_list = []\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __mmd2_loss(self, X_p_enc, X_f_enc):\n",
    "        sigma_var = self.sigma_var\n",
    "\n",
    "        # some constants\n",
    "        n_basis = 1024\n",
    "        gumbel_lmd = 1e+6\n",
    "        cnst = math.sqrt(1. / n_basis)\n",
    "        n_mixtures = sigma_var.size(0)\n",
    "        n_samples = n_basis * n_mixtures\n",
    "        batch_size, seq_len, nz = X_p_enc.size()\n",
    "\n",
    "        # gumbel trick to get masking matrix to uniformly sample sigma\n",
    "        # input: (batch_size*n_samples, nz)\n",
    "        # output: (batch_size, n_samples, nz)\n",
    "        def sample_gmm(W, batch_size):\n",
    "            U = torch.FloatTensor(batch_size*n_samples,\n",
    "                                  n_mixtures).uniform_().to(self.device)\n",
    "            sigma_samples = F.softmax(U * gumbel_lmd, dim=1).matmul(sigma_var)\n",
    "            W_gmm = W.mul(1. / sigma_samples.unsqueeze(1))\n",
    "            W_gmm = W_gmm.view(batch_size, n_samples, nz)\n",
    "            return W_gmm\n",
    "\n",
    "        W = Variable(torch.FloatTensor(batch_size*n_samples,\n",
    "                     nz).normal_(0, 1).to(self.device))\n",
    "        # batch_size x n_samples x nz\n",
    "        W_gmm = sample_gmm(W, batch_size)\n",
    "        # batch_size x nz x n_samples\n",
    "        W_gmm = torch.transpose(W_gmm, 1, 2).contiguous()\n",
    "        # batch_size x seq_len x n_samples\n",
    "        XW_p = torch.bmm(X_p_enc, W_gmm)\n",
    "        # batch_size x seq_len x n_samples\n",
    "        XW_f = torch.bmm(X_f_enc, W_gmm)\n",
    "        z_XW_p = cnst * torch.cat((torch.cos(XW_p), torch.sin(XW_p)), 2)\n",
    "        z_XW_f = cnst * torch.cat((torch.cos(XW_f), torch.sin(XW_f)), 2)\n",
    "        batch_mmd2_rff = torch.sum((z_XW_p.mean(1) - z_XW_f.mean(1))**2, 1)\n",
    "        return batch_mmd2_rff\n",
    "\n",
    "    def forward(self, X_p: torch.Tensor, X_f: torch.Tensor):\n",
    "        batch_size = X_p.size(0)\n",
    "\n",
    "        X_p_enc, _ = self.netD(X_p)\n",
    "        X_f_enc, _ = self.netD(X_f)\n",
    "        Y_pred_batch = self.__mmd2_loss(X_p_enc, X_f_enc)\n",
    "\n",
    "        return Y_pred_batch\n",
    "\n",
    "\n",
    "    def predict(self, ts):\n",
    "        dataset = HankelDataset(\n",
    "            ts, self.p_wnd_dim, self.f_wnd_dim, self.sub_dim)\n",
    "        dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                X_p, X_f = [batch[key].float().to(self.device)\n",
    "                            for key in ['X_p', 'X_f']]\n",
    "                pred_val = self.forward(X_p, X_f).cpu().detach().numpy()\n",
    "                preds.append(pred_val)\n",
    "        return np.concatenate(preds)\n",
    "\n",
    "    def fit(self, ts, start_epoch, svd_method, components, epoches: int = 100, lr: float = 1e-2, weight_clip: float = .1, weight_decay: float = 0., momentum: float = 0., dataset_name=None):\n",
    "        # print('***** Training *****')\n",
    "        # must be defined in fit() method\n",
    "        optG_adam = torch.optim.AdamW(\n",
    "            self.netG.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        # lr_scheduler_g = lr_scheduler.CosineAnnealingLR(optG, T_max=epoches, eta_min=3e-5)\n",
    "        optD_adam = torch.optim.AdamW(\n",
    "            self.netD.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        # lr_scheduler_d = lr_scheduler.CosineAnnealingLR(optD, T_max=epoches, eta_min=3e-5)\n",
    "        optD_rmsprop = torch.optim.RMSprop(\n",
    "            self.netD.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        optG_rmsprop = torch.optim.RMSprop(\n",
    "            self.netG.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        \n",
    "\n",
    "        dataset = HankelDataset(\n",
    "            ts, self.p_wnd_dim, self.f_wnd_dim, self.sub_dim)\n",
    "        dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "        sigma_list = median_heuristic(dataset.Y_hankel, beta=.5)\n",
    "        self.sigma_var = torch.FloatTensor(sigma_list).to(self.device)\n",
    "\n",
    "        # tbar = trange(epoches)\n",
    "\n",
    "        for epoch in range(start_epoch, epoches):\n",
    "            for batch in dataloader:\n",
    "                # Fit critic\n",
    "                for p in self.netD.parameters():\n",
    "                    p.requires_grad = True\n",
    "                for p in self.netD.rnn_enc_layer.parameters():\n",
    "                    p.data.clamp_(-weight_clip, weight_clip)\n",
    "                # (D_mmd2_mean, mmd2_real_mean, real_L2_loss, fake_L2_loss) = self._optimizeD(batch, optD)\n",
    "                # train after every 15 epochs\n",
    "                # if epoch % 15 == 0:\n",
    "                self._optimizeD(batch, optD_rmsprop)\n",
    "                # G_mmd2_mean = 0\n",
    "                # if np.random.choice(np.arange(self.critic_iters)) == 0:\n",
    "                # Fit generator\n",
    "                for p in self.netD.parameters():\n",
    "                    p.requires_grad = False  # to avoid computation\n",
    "                # G_mmd2_mean = self._optimizeG(batch, optG)\n",
    "                self._optimizeG(batch, optG_rmsprop)\n",
    "            \n",
    "            optD_adam.step()\n",
    "            optG_adam.step()          \n",
    "\n",
    "    def minimize(self, ts):\n",
    "        dataset = HankelDataset(\n",
    "            ts, self.p_wnd_dim, self.f_wnd_dim, self.sub_dim)\n",
    "        dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "        optG_losses = []\n",
    "        optD_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                X_p, X_f = [batch[key].float().to(self.device)\n",
    "                            for key in ['X_p', 'X_f']]\n",
    "                 batch_size = X_p.size(0)\n",
    "\n",
    "                # real data\n",
    "                X_f_enc, X_f_dec = self.netD(X_f)\n",
    "        \n",
    "                # fake data\n",
    "                noise = torch.FloatTensor(1, batch_size, self.RNN_hid_dim).uniform_(-1, 1).to(self.device)\n",
    "                # noise = torch.FloatTensor(1, batch_size, self.RNN_hid_dim).normal_(0, 1).to(self.device)\n",
    "                noise = Variable(noise)\n",
    "                Y_f = self.netG(X_p, X_f, noise)\n",
    "                Y_f_enc, Y_f_dec = self.netD(Y_f)\n",
    "                G_mmd2 = self.__mmd2_loss(X_f_enc, Y_f_enc)\n",
    "                optG_losses.append(G_mmd2.mean().data.item())\n",
    "\n",
    "                X_p_enc, X_p_dec = self.netD(X_p)\n",
    "                X_f_enc, X_f_dec = self.netD(X_f)\n",
    "        \n",
    "                # fake data\n",
    "                noise = torch.FloatTensor(1, batch_size, self.netG.RNN_hid_dim).uniform_(-1, 1).to(self.device)\n",
    "                # noise = torch.FloatTensor(1, batch_size, self.netG.RNN_hid_dim).normal_(0, 1).to(self.device)\n",
    "                noise = Variable(noise)  # total freeze netG\n",
    "                torch.no_grad()\n",
    "                Y_f = Variable(self.netG(X_p, X_f, noise).data)\n",
    "                Y_f_enc, Y_f_dec = self.netD(Y_f)\n",
    "        \n",
    "                # batchwise MMD2 loss between X_f and Y_f\n",
    "                D_mmd2 = self.__mmd2_loss(X_f_enc, Y_f_enc)\n",
    "        \n",
    "                # batchwise MMD loss between X_p and X_f\n",
    "                mmd2_real = self.__mmd2_loss(X_p_enc, X_f_enc)\n",
    "                optD_losses(mmd2_real.mean().item().data())\n",
    "        return np.mean(optG_losses) + np.mean(optD_losses)\n",
    "\n",
    "    def _optimizeG(self, batch, opt, lr_scheduler=None, grad_clip: int = 5):\n",
    "        X_p, X_f = [batch[key].float().to(self.device)\n",
    "                    for key in ['X_p', 'X_f']]\n",
    "        batch_size = X_p.size(0)\n",
    "\n",
    "        # real data\n",
    "        X_f_enc, X_f_dec = self.netD(X_f)\n",
    "\n",
    "        # fake data\n",
    "        noise = torch.FloatTensor(1, batch_size, self.RNN_hid_dim).uniform_(-1, 1).to(self.device)\n",
    "        # noise = torch.FloatTensor(1, batch_size, self.RNN_hid_dim).normal_(0, 1).to(self.device)\n",
    "        noise = Variable(noise)\n",
    "        Y_f = self.netG(X_p, X_f, noise)\n",
    "        Y_f_enc, Y_f_dec = self.netD(Y_f)\n",
    "\n",
    "        # batchwise MMD2 loss between X_f and Y_f\n",
    "        G_mmd2 = self.__mmd2_loss(X_f_enc, Y_f_enc)\n",
    "\n",
    "        # update netG\n",
    "        self.netG.zero_grad()\n",
    "        lossG = G_mmd2.mean()\n",
    "        # lossG = 0.0 * G_mmd2.mean()\n",
    "        self.loss_g_list.append(lossG.data.item())\n",
    "        lossG.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(self.netG.parameters(), grad_clip)\n",
    "\n",
    "        opt.step()\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        # return G_mmd2.mean().data.item()\n",
    "\n",
    "    def _optimizeD(self, batch, opt, lr_scheduler=None, grad_clip: int = 5):\n",
    "        X_p, X_f, Y_true = [batch[key].float().to(self.device)\n",
    "                            for key in ['X_p', 'X_f', 'Y']]\n",
    "        batch_size = X_p.size(0)\n",
    "\n",
    "        # real data\n",
    "        X_p_enc, X_p_dec = self.netD(X_p)\n",
    "        X_f_enc, X_f_dec = self.netD(X_f)\n",
    "\n",
    "        # fake data\n",
    "        noise = torch.FloatTensor(1, batch_size, self.netG.RNN_hid_dim).uniform_(-1, 1).to(self.device)\n",
    "        # noise = torch.FloatTensor(1, batch_size, self.netG.RNN_hid_dim).normal_(0, 1).to(self.device)\n",
    "        noise = Variable(noise)  # total freeze netG\n",
    "        torch.no_grad()\n",
    "        Y_f = Variable(self.netG(X_p, X_f, noise).data)\n",
    "        Y_f_enc, Y_f_dec = self.netD(Y_f)\n",
    "\n",
    "        # batchwise MMD2 loss between X_f and Y_f\n",
    "        D_mmd2 = self.__mmd2_loss(X_f_enc, Y_f_enc)\n",
    "\n",
    "        # batchwise MMD loss between X_p and X_f\n",
    "        mmd2_real = self.__mmd2_loss(X_p_enc, X_f_enc)\n",
    "\n",
    "        # reconstruction loss\n",
    "        real_L2_loss = torch.mean((X_f - X_f_dec)**2)\n",
    "        fake_L2_loss = torch.mean((Y_f - Y_f_dec)**2)\n",
    "\n",
    "        # update netD\n",
    "        self.netD.zero_grad()\n",
    "        lossD = D_mmd2.mean() - self.lambda_ae * (real_L2_loss + fake_L2_loss) - \\\n",
    "            self.lambda_real * mmd2_real.mean()\n",
    "        lossD = -lossD\n",
    "        self.loss_d_list.append(lossD.data.item())\n",
    "        lossD.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(self.netD.parameters(), grad_clip)\n",
    "\n",
    "        opt.step()\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "#        return D_mmd2.mean().data.item() + mmd2_real.mean().data.item() +  real_L2_loss.data.item() + fake_L2_loss.data.item()\n",
    "\n",
    "\n",
    "def svd_wrapper(Y, k, method='svds'):\n",
    "    if method == 'svds':\n",
    "        Ut, St, Vt = svds(Y, k)\n",
    "        idx = np.argsort(St)[::-1]\n",
    "        St = St[idx]  # have issue with sorting zero singular values\n",
    "        Ut, Vt = svd_flip(Ut[:, idx], Vt[idx])\n",
    "    elif method == 'random':\n",
    "        Ut, St, Vt = randomized_svd(Y, k, random_state=0)\n",
    "    else:\n",
    "        Ut, St, Vt = np.linalg.svd(Y, full_matrices=False)\n",
    "        # now truncate it to k\n",
    "        Ut = Ut[:, :k]\n",
    "        St = np.diag(St[:k])\n",
    "        Vt = Vt[:k, :]\n",
    "\n",
    "    return Ut, St, Vt\n",
    "\n",
    "\n",
    "def get_reduced_data(dataset, components, svd_method):\n",
    "    print(f'***** Original dataset shape: {dataset.shape} *****')\n",
    "    X, _, _ = svd_wrapper(dataset, components, method=svd_method)\n",
    "    print(f'***** Reduced dataset shape: {X.shape} *****')\n",
    "    return X\n",
    "\n",
    "\n",
    "def train_and_pred_dataset(dataset, dataset_name, svd_method, components, preload_model=False):\n",
    "    '''If dataset name is not None, then save the model dict to file after each epoch. If preload_model is True, then load the model from file and continue training'''\n",
    "    dimension = dataset.shape[1]\n",
    "    start_epoch = 0\n",
    "    model = KL_CPD(dimension).to(device)\n",
    "\n",
    "\n",
    "    model.fit(dataset, dataset_name=dataset_name, start_epoch=start_epoch,\n",
    "              svd_method=svd_method, components=components)\n",
    "    predictions = model.predict(dataset)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed763b7-333f-4323-9381-2417e7a96d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter space range\n",
    "params = {\n",
    "    \"lambda_ae\": hp.uniform(\"lambda_ae\", 0.000001, 0.1),\n",
    "    \"lambda_real\": hp.uniform(\"lambda_real\", 0.000001, 0.1),\n",
    "    \"p_wnd_dim\": scope.int(hp.quniform(\"p_wnd_dim\", 1, 5, 1)),\n",
    "    \"f_wnd_dim\": scope.int(hp.quniform(\"f_wnd_dim\", 1, 5, 1)),\n",
    "    \"sub_dim\": scope.int(hp.quniform(\"sub_dim\", 1, 5, 1)),\n",
    "    \"RNN_hid_dim\": scope.int(hp.quniform(\"RNN_hid_dim\", 1, 15, 1)),\n",
    "    \"svd_method\": hp.choice(\"svd_method\", [\"svds\", \"random\", \"regular\", None]),\n",
    "    \"component\": hp.choice(\"component\", [1, 2]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bac8b8-a98d-4f8f-99bc-dc29b484ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_model_name = f\"PRE_LOAD_{dataset_name.upper()}_MODEL\"\n",
    "# convert to boolean as env variable are always string\n",
    "PRE_LOAD_PROTEIN_1FME_MODEL = os.getenv(preload_model_name, False) == 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a06ac9-80d9-4ca2-be6f-104674f7c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimization function to be minimized\n",
    "def optimize(params):\n",
    "\n",
    "    data = get_coordinates(dataset_name)\n",
    "    if params['svd_method']:\n",
    "        data = get_reduced_data(data, params['components'], params['svd_method'])\n",
    "    \n",
    "    dimension = data.shape[1]\n",
    "    model = KL_CPD(\n",
    "        dimension,\n",
    "        lambda_ae = params['lambda_ae'],\n",
    "        lambda_real = params['lambda_real'],\n",
    "        p_wnd_dim = params['p_wnd_dim'],\n",
    "        f_wnd_dim = params['f_wnd_dim'],\n",
    "        sub_dim = params['sub_dim'],\n",
    "        RNN_hid_dim = params['RNN_hid_dim']\n",
    "    ).to(device)\n",
    "\n",
    "    return  model.minimize(data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5ac56-c15b-44bd-8512-f870f4b3951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_function = partial(\n",
    "    optimize\n",
    " )\n",
    " # initialize trials to keep logging information\n",
    " trials = Trials()\n",
    "\n",
    " # run hyperopt with 10 trials\n",
    " hopt = fmin(\n",
    "     fn=optimization_function,\n",
    "     space=param_space,\n",
    "     algo=tpe.suggest,\n",
    "     max_evals=10,\n",
    "     trials=trials\n",
    " )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
